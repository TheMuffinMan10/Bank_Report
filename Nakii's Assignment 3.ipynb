{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheMuffinMan10/Bank_Report/blob/main/Nakii's%20Assignment%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3: Bayesian Network Analysis\n",
        "\n",
        "**Student Name:** Nakedi Lebeya  \n",
        "**Course:** Data Analytics\n",
        "\n",
        "**Module Code:** EDAB2724\n",
        "  \n",
        "\n",
        "**Dataset Source:**  \n",
        "[Data modelling of subsistence retail consumer purchase behavior in South Africa](https://www.sciencedirect.com/science/article/pii/S2352340922003043)\n",
        "\n",
        "\n",
        "\n",
        "##Notebook Structure\n",
        "\n",
        "This notebook follows the **CRISP-DM process** for structured data science analysis:\n",
        "\n",
        "1. **Business Understanding**  \n",
        "   - Define the project objectives and business questions.  \n",
        "   - Explain why Bayesian Networks are useful for understanding purchase intention.\n",
        "\n",
        "2. **Data Understanding**  \n",
        "   - Explore the dataset to identify key variables.  \n",
        "   - Visualize correlations using heatmaps to understand relationships among variables.  \n",
        "   - Identify potential data quality issues and patterns.\n",
        "\n",
        "3. **Data Preparation**  \n",
        "   - Clean and preprocess the data (remove invalid values, handle missing data).  \n",
        "   - Perform feature engineering to create constructs (e.g. Convenience, Price Sensitivity).  \n",
        "   - Normalize the data to prepare for modeling.\n",
        "\n",
        "4. **Modeling**  \n",
        "   - Build the **Expert Bayesian Network** (DAG 2.1) based on domain knowledge.  \n",
        "   - Build the **Learned Bayesian Network** (DAG 2.2) using Hill-Climb Search and AIC scoring.  \n",
        "   - Visualize both networks.\n",
        "\n",
        "5. **Evaluation**  \n",
        "   - Compare the Expert and Learned DAGs using a confusion matrix and visual DAGcomparison.  \n",
        "   - Highlight common and unique edges between the two models.  \n",
        "\n",
        "6. **Parameter Learning**  \n",
        "   - Learn Conditional Probability Distributions (CPDs) using Maximum Likelihood Estimation (MLE).  \n",
        "   - Examine CPDs for key variables to understand probabilistic relationships.\n",
        "\n",
        "7. **Insights & Conclusions**  \n",
        "   - Infer patterns and trends from the Bayesian Network.  \n",
        "   - Discuss actionable insights for subsistence retailers or marketing strategies.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLqcfvefeopW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Business Understanding\n",
        "\n",
        "The first phase of the CRISP-DM process focuses on understanding the **business objectives** and defining a clear plan to solve the problem. In this project, the goal is to **build and compare Bayesian Network models** to determine the relationships between factors that influence **purchase intention** among small-scale consumers, as indicated in dataset we were provided with.\n",
        "\n",
        "From a business perspective, understanding how **demographic, behavioral, and perception-based variables** interact to affect purchase decisions is crucial. This understadning can help businesses design more effective marketing,commmunication,and pricing strategies that focues on improving consumer engagement and promoting maintainable purchasing behavior.\n",
        "\n",
        "To accomplish this, the following objectives were defined:\n",
        "\n",
        "- Identify and model the key factors that influence **purchase intention**.  \n",
        "- Use **Bayesian Networks** to visualize dependencies among variables.  \n",
        "- Compare an expert built network which is based on domain knowledge with a data-driven network (learned using Hill-Climb Search and Akaike Information Criterion).  \n",
        "- Evaluate both DAGs to see how well the learned model reflects real-world relationships.  \n",
        "- Learn the conditional probability parameters to understand the likelihood of outcomes under different conditions.\n",
        "\n",
        "The essential resource used for this project include:  \n",
        "- The dataset obtained from the **ScienceDirect article** provided in the assignment brief.  \n",
        "\n",
        "\n",
        "This phase sets up a foundation for the rest of the project by aligning technical modeling tasks with the overall business goal, getting **data-driven insights** into the factors that shape consumer purchase behavior.\n"
      ],
      "metadata": {
        "id": "BghhooUwJzEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Packages and Libraries"
      ],
      "metadata": {
        "id": "RumLP3VMJb_F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10c3eb19",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd06320c-9a99-4a4b-dd7d-a2965e041b90"
      },
      "source": [
        "!pip install bnlearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlearn\n",
            "  Downloading bnlearn-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pgmpy==0.1.25 (from bnlearn)\n",
            "  Downloading pgmpy-0.1.25-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from bnlearn) (3.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from bnlearn) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from bnlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from bnlearn) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from bnlearn) (4.67.1)\n",
            "Collecting ismember (from bnlearn)\n",
            "  Downloading ismember-1.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from bnlearn) (1.6.1)\n",
            "Collecting funcsigs (from bnlearn)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from bnlearn) (0.14.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.12/dist-packages (from bnlearn) (0.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from bnlearn) (25.0)\n",
            "Collecting df2onehot (from bnlearn)\n",
            "  Downloading df2onehot-1.0.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from bnlearn) (2025.3.0)\n",
            "Collecting pypickle (from bnlearn)\n",
            "  Downloading pypickle-2.0.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from bnlearn) (0.9.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from bnlearn) (7.7.1)\n",
            "Collecting datazets>=1.1.2 (from bnlearn)\n",
            "  Downloading datazets-1.1.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting setgraphviz>=1.0.3 (from bnlearn)\n",
            "  Downloading setgraphviz-1.0.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting lingam (from bnlearn)\n",
            "  Downloading lingam-1.11.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pyarrow<20.0.0 in /usr/local/lib/python3.12/dist-packages (from bnlearn) (18.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pgmpy==0.1.25->bnlearn) (1.16.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from pgmpy==0.1.25->bnlearn) (3.2.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pgmpy==0.1.25->bnlearn) (2.8.0+cu126)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pgmpy==0.1.25->bnlearn) (1.5.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.12/dist-packages (from pgmpy==0.1.25->bnlearn) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from datazets>=1.1.2->bnlearn) (2.32.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->bnlearn) (2.9.0.post0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->bnlearn) (3.0.15)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from lingam->bnlearn) (0.21)\n",
            "Collecting pygam (from lingam->bnlearn)\n",
            "  Downloading pygam-0.10.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting psy (from lingam->bnlearn)\n",
            "  Downloading psy-0.0.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting semopy (from lingam->bnlearn)\n",
            "  Downloading semopy-2.3.11.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->bnlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->bnlearn) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->bnlearn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->bnlearn) (1.0.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets->bnlearn) (6.5.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->bnlearn)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets->bnlearn) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->bnlearn) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (6.5.7)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.12/dist-packages (from psy->lingam->bnlearn) (4.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->datazets>=1.1.2->bnlearn) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->datazets>=1.1.2->bnlearn) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->datazets>=1.1.2->bnlearn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->datazets>=1.1.2->bnlearn) (2025.10.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from semopy->lingam->bnlearn) (1.13.3)\n",
            "Collecting numdifftools (from semopy->lingam->bnlearn)\n",
            "  Downloading numdifftools-0.9.41-py2.py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (4.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy==0.1.25->bnlearn) (3.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->bnlearn) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->bnlearn) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->bnlearn) (5.9.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.3.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->bnlearn) (0.7.0)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from progressbar2->psy->lingam->bnlearn) (3.9.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->bnlearn) (0.2.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->semopy->lingam->bnlearn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pgmpy==0.1.25->bnlearn) (3.0.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->bnlearn) (4.5.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (4.25.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (25.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.28.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2.14.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (2.23)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (4.11.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.3.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->bnlearn) (1.4.0)\n",
            "Downloading bnlearn-0.12.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pgmpy-0.1.25-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datazets-1.1.3-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.4/190.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setgraphviz-1.0.3-py3-none-any.whl (7.9 kB)\n",
            "Downloading df2onehot-1.0.8-py3-none-any.whl (15 kB)\n",
            "Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Downloading ismember-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading lingam-1.11.0-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypickle-2.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading psy-0.0.1-py2.py3-none-any.whl (38 kB)\n",
            "Downloading pygam-0.10.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numdifftools-0.9.41-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: semopy\n",
            "  Building wheel for semopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for semopy: filename=semopy-2.3.11-py3-none-any.whl size=1659682 sha256=80dfb5e2faddcb5e7884ce872e143a1f11cf6e46a21c8235311002bb1b1d65a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/24/8b/be911b059a61f490f38425eb19bf2fed470a5ead97228e8255\n",
            "Successfully built semopy\n",
            "Installing collected packages: funcsigs, pypickle, jedi, ismember, setgraphviz, numdifftools, pygam, psy, datazets, semopy, pgmpy, df2onehot, lingam, bnlearn\n",
            "Successfully installed bnlearn-0.12.0 datazets-1.1.3 df2onehot-1.0.8 funcsigs-1.0.2 ismember-1.1.0 jedi-0.19.2 lingam-1.11.0 numdifftools-0.9.41 pgmpy-0.1.25 psy-0.0.1 pygam-0.10.1 pypickle-2.0.1 semopy-2.3.11 setgraphviz-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f77b61a"
      },
      "source": [
        "import bnlearn as bn\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"bnlearn version is:\", bn.__version__)\n",
        "print(\"pandas version is:\", pd.__version__)"
      ],
      "metadata": {
        "id": "eYV4aTt6JoPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import your data"
      ],
      "metadata": {
        "id": "X6yevZY8Jlca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Link your data via Google sheets use the following code:\n",
        "import gdown #for importing data from Google Drive\n",
        "import os #zip unzip from Google Drive\n",
        "\n",
        "'''\n",
        "#https://docs.google.com/spreadsheets/d/copythispartofthelink/edit?usp=sharing\n",
        "#Please paste your link below this link and copy the right part of the link in below\n",
        "\n",
        "#Important\n",
        "#Make sure that your Google Sheet is Public, anyone with link can access, otherwise it will not work.\n",
        "\n",
        "spreadsheet_file_id = 'copythispartofthelink'\n",
        "output_filename = 'Youroutputfile'\n",
        "\n",
        "# Download the single spreadsheet file\n",
        "gdown.download(id=spreadsheet_file_id, output=output_filename, quiet=False)\n",
        "\n",
        "print(f\"Spreadsheet downloaded as {output_filename}\")'''\n"
      ],
      "metadata": {
        "id": "5lI4AT0ELz2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_data.csv' with the actual path to your dataset\n",
        "retail = pd.read_excel(r\"/content/sample_data/Subsistence Retail Consumer Data.xlsx\")\n",
        "retail.head(10)\n"
      ],
      "metadata": {
        "id": "y0e36MpVJlsZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TEwrA_oQMQOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA / Feature engineer your data"
      ],
      "metadata": {
        "id": "xs6VhoEtQC2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Data Understanding\n",
        "\n",
        "In this step, We explored the dataset to understand its structure and format format  \n",
        "The goal is to familiarize with the data, identify missing values, and get insights into the types of variables we have.\n",
        "\n",
        "The following actions were taken:\n",
        "- Column names were cleaned to make them Python-friendly.  \n",
        "- The dataset’s structure and variable types were reviewed using **info()**.  \n",
        "- Descriptive statistics were checked to understand distributions of numerical features.  \n",
        "- Missing values were checked to check whether the dataset is complete.  \n",
        "\n",
        "This step helps identify early data quality issues before further processing and modeling.\n"
      ],
      "metadata": {
        "id": "YGp7UWiUQ_Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make all column names Python-friendly\n",
        "retail.columns = [col.strip().replace(\" \", \"_\") for col in retail.columns]\n",
        "\n",
        "print(retail.columns)"
      ],
      "metadata": {
        "id": "6vLrjBzH7VRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get basic information about the DataFrame\n",
        "print(\"\\nDataFrame Info:\")\n",
        "retail.info()\n",
        "\n",
        "#Get descriptive statistics for numeric columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(retail.describe())\n",
        "\n",
        "#Check for missing values\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "retail.isnull().sum()\n"
      ],
      "metadata": {
        "id": "_uvcxYcPQHu7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Data Preparation\n",
        "\n",
        "The data preparation phase focuses on cleaning and transforming the dataset to make it fit for modeling.  \n",
        "It includes handling missing or invalid values, combining related survey items into constructs, and scaling values for consistent analysis.\n",
        "\n",
        "Key actions takenn in this step:\n",
        "-  Data errors such as invalid Likert-scale responses and incorrect demographic values are removed.  \n",
        "- A clean dataset named **(clean_retail)** is created containing only valid observations.  \n",
        "- Performed **feature engineering** by averaging related items into broader constructs ( **Empathy**, **Convenience**, **Price Sensitivity**, and **Customer Trust**).  \n",
        "-  Only relevant demographic and construct variables for modeling are kept.  \n",
        "- Applied **MinMaxScaler** to normalize variable values between 0 and 1.\n",
        "\n",
        "This cleaned and preprocessed dataset is now ready to be used for Bayesian Network structure learning.\n"
      ],
      "metadata": {
        "id": "0EyKMnRqSQM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers"
      ],
      "metadata": {
        "id": "1YWvo_HG7dWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Clean retail data\n",
        "\n",
        "def get_data_errors(df):\n",
        "    \"\"\"\n",
        "    Get dataframe containing ONLY data errors (not statistical outliers)\n",
        "    Invalid Likert scale values (outside 1-5)\n",
        "    Invalid demographic categories\n",
        "    Missing values\n",
        "    \"\"\"\n",
        "\n",
        "    # Gets Likert scale errors (values outside 1-5)\n",
        "    likert_columns = [col for col in df.columns if any(\n",
        "        col.startswith(prefix) for prefix in ['E', 'C', 'PS', 'PE', 'PPQ', 'CT', 'PV', 'PI']\n",
        "    )]\n",
        "\n",
        "    likert_error_mask = pd.Series([False] * len(df), index=df.index)\n",
        "    for col in likert_columns:\n",
        "        if col in df.columns:\n",
        "            likert_error_mask |= (df[col] < 1) | (df[col] > 5)\n",
        "\n",
        "    # Gets demographic errors\n",
        "    demographic_checks = {\n",
        "        'Gender': [1, 2, 3],\n",
        "        'Age': [1, 2, 3, 4, 5],\n",
        "        'Marital_Status': [1, 2, 3],\n",
        "        'Employment_Status': [1, 2],\n",
        "        'Level_of_Education': [1, 2, 3, 4, 5],\n",
        "        'Regular_Customer': [1, 2, 3],\n",
        "        'Shopping_Frequency': [1, 2, 3, 4, 5]\n",
        "    }\n",
        "\n",
        "    demographic_error_mask = pd.Series([False] * len(df), index=df.index)\n",
        "    for col, valid_values in demographic_checks.items():\n",
        "        if col in df.columns:\n",
        "            demographic_error_mask |= ~df[col].isin(valid_values)\n",
        "\n",
        "    # Get missing values\n",
        "    missing_error_mask = df.isnull().any(axis=1)\n",
        "\n",
        "    # Combine all error masks\n",
        "    all_errors_mask = likert_error_mask | demographic_error_mask | missing_error_mask\n",
        "\n",
        "    # Create error dataframe\n",
        "    error_df = df[all_errors_mask].copy()\n",
        "\n",
        "    # Add columns to identify error types\n",
        "    error_df['has_likert_error'] = error_df.index.isin(df[likert_error_mask].index)\n",
        "    error_df['has_demographic_error'] = error_df.index.isin(df[demographic_error_mask].index)\n",
        "    error_df['has_missing_values'] = error_df.index.isin(df[missing_error_mask].index)\n",
        "\n",
        "    # Add details about which columns have errors\n",
        "    if len(error_df) > 0:\n",
        "        # Identify problematic Likert columns\n",
        "        error_df['problematic_likert_cols'] = error_df.apply(\n",
        "            lambda row: ', '.join([col for col in likert_columns\n",
        "                                  if col in df.columns and (row[col] < 1 or row[col] > 5)])\n",
        "                        if pd.notna(row.name) else '',\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Identify problematic demographic columns\n",
        "        error_df['problematic_demographic_cols'] = error_df.apply(\n",
        "            lambda row: ', '.join([col for col, valid_values in demographic_checks.items()\n",
        "                                  if col in df.columns and row[col] not in valid_values])\n",
        "                        if pd.notna(row.name) else '',\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Identify columns with missing values\n",
        "        error_df['missing_value_cols'] = error_df.apply(\n",
        "            lambda row: ', '.join([col for col in df.columns if pd.isnull(row[col])]),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    return error_df, all_errors_mask\n",
        "\n",
        "\n",
        "def clean_retail_data(df, display_errors=True):\n",
        "    # Get data errors\n",
        "    error_df, error_mask = get_data_errors(df)\n",
        "\n",
        "    # Create clean dataframe\n",
        "    clean_df = df[~error_mask].copy()\n",
        "    print(f\"\\nCleaned dataset: {len(clean_df)} rows ({len(clean_df)/len(df)*100:.2f}%)\")\n",
        "\n",
        "\n",
        "    # Display error details if requested\n",
        "    if display_errors and len(error_df) > 0:\n",
        "        print(\"ERROR DETAILS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Show summary of errors by type\n",
        "        error_summary = error_df[['has_likert_error', 'has_demographic_error',\n",
        "                                  'has_missing_values']].sum()\n",
        "        print(\"\\nError counts by type:\")\n",
        "        print(error_summary)\n",
        "        print(\"Sample of error rows (first 10):\")\n",
        "\n",
        "        display_cols = ['has_likert_error', 'has_demographic_error', 'has_missing_values',\n",
        "                       'problematic_likert_cols', 'problematic_demographic_cols',\n",
        "                       'missing_value_cols']\n",
        "        print(error_df[display_cols].head(10))\n",
        "\n",
        "    return clean_df, error_df\n",
        "\n",
        "# Clean the retail dataframe\n",
        "clean_retail, error_records = clean_retail_data(retail, display_errors=True)\n",
        "\n",
        "# Display of all error records (if they exist))\n",
        "print(\"All error records \")\n",
        "error_records\n",
        "\n",
        "print(\"Summary\")\n",
        "print(f\"Original 'retail' dataframe: {len(retail)} rows\")\n",
        "print(f\"'error_records' dataframe: {len(error_records)} rows (removed)\")\n",
        "print(f\"'clean_retail' dataframe: {len(clean_retail)} rows (to use for analysis)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W85ZnISoQH26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "pik-hH657lQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Construct groupings\n",
        "constructs = {\n",
        "    # Section B: Measurment instruments\n",
        "    \"Empathy\": [\"E1\", \"E2\", \"E3\", \"E4\"],\n",
        "    \"Convenience\": [\"C1\", \"C2\", \"C3\"],\n",
        "    \"Price_Sensitivity\": [\"PS1\", \"PS2\", \"PS3\"],\n",
        "    \"Physical_Environment\": [\"PE1\", \"PE2\", \"PE3\", \"PE4\", \"PE5\", \"PE6\"],\n",
        "    \"Perceived_Product_Quality\": [\"PPQ1\", \"PPQ2\", \"PPQ3\", \"PPQ4\"],\n",
        "    \"Customer_Trust\": [\"CT1\", \"CT2\", \"CT3\", \"CT4\", \"CT5\", \"CT6\", \"CT7\"],\n",
        "    \"Perceived_Value\": [\"PV1\", \"PV2\", \"PV3\"],\n",
        "    \"Purchase_Intention\": [\"PI1\", \"PI2\", \"PI3\", \"PI4\"]\n",
        "}\n",
        "\n",
        "\n",
        "# Compute the mean of each construct\n",
        "for construct, items in constructs.items():\n",
        "    available_items = [col for col in items if col in retail.columns]\n",
        "    if available_items:\n",
        "        retail[construct] = retail[available_items].mean(axis=1).round(2)\n",
        "    else:\n",
        "        print(f\"Warning: None of the items for '{construct}' were found in the DataFrame.\")\n",
        "\n",
        "retail.head(10)"
      ],
      "metadata": {
        "id": "TXpAJaq4QIZ_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only demographics + constructs individual survey items are removed\n",
        "columns_to_keep = [\n",
        "    # Demographics\n",
        "    'Gender', 'Age', 'Marital_Status', 'Employment_Status',\n",
        "    'Level_of_Education', 'Regular_Customer', 'Shopping_frequency',\n",
        "\n",
        "    # Constructs (aggregated)\n",
        "    'Empathy', 'Convenience', 'Price_Sensitivity',\n",
        "    'Physical_Environment', 'Perceived_Product_Quality',\n",
        "    'Customer_Trust', 'Perceived_Value', 'Purchase_Intention'\n",
        "]\n",
        "\n",
        "# Create clean dataset\n",
        "clean_retail_final = retail[columns_to_keep].copy()\n",
        "clean_retail_final\n",
        "print(f\"Original columns: {len(clean_retail.columns)}\")\n",
        "print(f\"Final columns: {len(clean_retail_final.columns)}\")\n",
        "retail = clean_retail_final\n"
      ],
      "metadata": {
        "id": "rWGun4x4-zpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retail"
      ],
      "metadata": {
        "id": "tqqVvkzmfzQJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# Fit and transform the data, then rebuild the DataFrame with same columns & index\n",
        "retail_clean = pd.DataFrame(\n",
        "    scaler.fit_transform(retail),\n",
        "    columns=retail.columns,\n",
        "    index=retail.index\n",
        ")\n",
        "retail = retail_clean\n",
        "retail_clean.head(10)"
      ],
      "metadata": {
        "id": "SxeYHg6xDpvx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisations"
      ],
      "metadata": {
        "id": "gkRGW383iXUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlations with 'Purchase_intention'\n",
        "correlations = retail.corr(numeric_only=True)['Purchase_Intention'].sort_values(ascending=False)\n",
        "\n",
        "# Convert to DataFrame for heatmap\n",
        "corr_df = pd.DataFrame(correlations).T\n",
        "\n",
        "plt.figure(figsize=(10, 2))\n",
        "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation with Purchase_intention\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Ctq7OPMiaja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Basic heatmap (includes all the factors)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(retail.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Retail Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MHgfhIMEjqf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Modeling\n",
        "\n",
        "In this phase, two Bayesian Network models are built:\n",
        "\n",
        "1. The **Expert Model (DAG 2.1)** — this model is constructed using intuition knowledge about how different variables influence each other.\n",
        "2. **Learned Model (DAG 2.2)** — this one automatically learnes from the dataset using the **Hill-Climb Search** algorithm with the **AIC score**.\n",
        "\n",
        "Both models are visualized to compare their network structures and edges relationships between variables.\n"
      ],
      "metadata": {
        "id": "-2GGwlKeWE9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build your Bayesian Networks (Structure Learning)"
      ],
      "metadata": {
        "id": "aUU9ScZnPr6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Parent nodes: Age, Level of Education, Physical Environment, Convenience, Perceived Product Quality, Customer Trust, Regular Customer\n",
        "\n",
        "####Child nodes: Marital Status, Employment Status, Price Sensitivity, Customer Trust, Perceived Product Quality, Purchase Intention"
      ],
      "metadata": {
        "id": "cmAJTZQop822"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bnlearn as bn\n",
        "\n",
        "#expert DAG edges\n",
        "edges = [\n",
        "    ('Perceived_Product_Quality', 'Purchase_Intention'),   # PPQ >>>PI\n",
        "    ('Perceived_Value', 'Purchase_Intention'),            # PV >>> PI\n",
        "    ('Physical_Environment', 'Convenience'),             # PE >>> Conv\n",
        "    ('Customer_Trust', 'Perceived_Value'),               # CT -> PV\n",
        "    ('Perceived_Product_Quality', 'Customer_Trust'),     # PPQ >>> CT\n",
        "    ('Level_of_Education', 'Customer_Trust'),            # E >>CT\n",
        "    ('Perceived_Product_Quality', 'Price_Sensitivity'),  # PPQ >> PS\n",
        "    ('Customer_Trust', 'Price_Sensitivity'),             # CT >> PS\n",
        "    ('Age', 'Price_Sensitivity'),                        # Age >> PS\n",
        "    ('Employment_Status', 'Price_Sensitivity'),          # EMP >>PS\n",
        "    ('Regular_Customer', 'Customer_Trust'),             # RC >> CT\n",
        "    ('Level_of_Education', 'Employment_Status'),         # EDU >>>EMP\n",
        "    ('Age', 'Marital_Status'),                           # Age >> MS\n",
        "    ('Convenience', 'Perceived_Product_Quality')         # CV >>PPQ\n",
        "]\n",
        "\n",
        "#Create DAG\n",
        "expert_model = bn.make_DAG(edges)\n",
        "\n",
        "#Node color mapping\n",
        "node_colors = {\n",
        "    'Age': 'pink',\n",
        "    'Marital Status': 'pink',\n",
        "    'Employment Status': 'pink',\n",
        "    'Education level': 'pink',\n",
        "    'Regular Customer': 'pink',\n",
        "    'Convenience': 'blue',\n",
        "    'Price Sensitivity': 'red',\n",
        "    'Physical Environment': 'teal',\n",
        "    'Perceived Product Quality': 'orange',\n",
        "    'Customer Trust': 'purple',\n",
        "    'Perceived Value': 'turquoise',\n",
        "    'Purchase Intention': 'whitesmoke'\n",
        "}\n",
        "\n",
        "#Plots the DAG\n",
        "'''bn.plot(expert_model,params_static={\n",
        "              'node_color': '#B3A7FF',\n",
        "              'layout': 'spring_layout',\n",
        "              'figsize': (14, 10),\n",
        "              'font_size': 14,\n",
        "              'font_family': 'sans-serif',\n",
        "              'edge_alpha': 0.7,\n",
        "              'node_size': 3500,\n",
        "              'arrowsize': 25\n",
        "            }\n",
        ")\n",
        "''''\n"
      ],
      "metadata": {
        "id": "1hogJLJ3LsIb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Extract expert DAG\n",
        "G_expert = expert_model['model']\n",
        "\n",
        "# Use a layout\n",
        "pos = nx.kamada_kawai_layout(G_expert)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G_expert, pos, node_color='#B3A7FF', node_size=1500, alpha=0.5)\n",
        "nx.draw_networkx_labels(G_expert, pos, font_color='black', font_size=10, font_weight='bold')\n",
        "\n",
        "# Draw edges one by one to avoid StopIteration errors\n",
        "for edge in G_expert.edges():\n",
        "    nx.draw_networkx_edges(\n",
        "        G_expert,\n",
        "        pos,\n",
        "        edgelist=[edge],\n",
        "        arrows=True,\n",
        "        arrowstyle='-|>',\n",
        "        arrowsize=20,\n",
        "        width=2,\n",
        "        edge_color='navy'\n",
        "    )\n",
        "\n",
        "# Title and axis\n",
        "plt.title(\"Expert DAG\", fontsize=14, fontweight='bold', pad=20)\n",
        "plt.axis('off')\n",
        "\n",
        "# Legend\n",
        "node_patch = mpatches.Patch(color='#B3A7FF', label='Node')\n",
        "edge_patch = mpatches.Patch(color='navy', label='Directed Edge')\n",
        "plt.legend(handles=[node_patch, edge_patch], loc='upper left', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vSGi2tYTdRJm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bn_model = bn.structure_learning.fit(\n",
        "    retail,\n",
        "    methodtype='hc',\n",
        "    scoretype='aic',\n",
        "    max_iter=500,\n",
        "    tabu_length=50\n",
        ")\n",
        "\n",
        "#Shows the edges learned\n",
        "print(\"Learned DAG edges:\")\n",
        "#print(model['model'].edges())\n",
        "'''\n",
        "#Visualize the DAG\n",
        "figsize=(6,3)\n",
        "bn.plot(\n",
        "    bn_model,\n",
        "    interactive=False,\n",
        "    node_color='#2ecc71',\n",
        "\n",
        ")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "S7HLGk8PR5xh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Extract expert DAG\n",
        "G_learned = bn_model['model']\n",
        "\n",
        "# Use a layout\n",
        "pos = nx.kamada_kawai_layout(G_learned)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G_learned, pos, node_color='#B3A7FF', node_size=1500, alpha=0.5)\n",
        "nx.draw_networkx_labels(G_learned, pos, font_color='black', font_size=10, font_weight='bold')\n",
        "\n",
        "# we draw edges one by one\n",
        "for edge in G_learned.edges():\n",
        "    nx.draw_networkx_edges(\n",
        "        G_learned,\n",
        "        pos,\n",
        "        edgelist=[edge],\n",
        "        arrows=True,\n",
        "        arrowstyle='-|>',\n",
        "        arrowsize=20,\n",
        "        width=2,\n",
        "        edge_color='navy'\n",
        "    )\n",
        "\n",
        "# Title and axis\n",
        "plt.title(\"Learned DAG\", fontsize=14, fontweight='bold', pad=20)\n",
        "plt.axis('off')\n",
        "\n",
        "# Legend\n",
        "node_patch = mpatches.Patch(color='#B3A7FF', label='Node')\n",
        "edge_patch = mpatches.Patch(color='navy', label='Directed Edge')\n",
        "plt.legend(handles=[node_patch, edge_patch], loc='upper left', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qVVnLH5haeOI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = bn.independence_test(bn_model, retail, alpha=0.05, prune=False)\n",
        "model1"
      ],
      "metadata": {
        "id": "siVqMG5ER58c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Evaluation\n",
        "\n",
        "In this step, we compare the **Expert DAG** and the **Learned DAG** to see how they align.\n",
        "\n",
        "We use:\n",
        "- A **confusion matrix** to measure edge overlaps and differences.\n",
        "- **Visual comparison plots (DAGs)** showing common and unique edges between the two models.\n",
        "\n",
        "This helps assess how well the learned model reflects expert understanding and whether the algorithm discovered any new relevnt  relationships.\n"
      ],
      "metadata": {
        "id": "B9A-EEJBWlrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare your 2 Bayesian Networks"
      ],
      "metadata": {
        "id": "rL_YcTT2P8m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#expert DAG edges from 2.1\n",
        "expert_edges = list(G_expert.edges())\n",
        "\n",
        "#learned DAG edges from 2.2\n",
        "learned_edges = list(bn_model['model'].edges())\n",
        "\n",
        "#confusion matrix\n",
        "expert_set = set(expert_edges)\n",
        "learned_set = set(learned_edges)\n",
        "\n",
        "# Calculate categories\n",
        "both = expert_set & learned_set\n",
        "only_expert = expert_set - learned_set\n",
        "only_learned = learned_set - expert_set\n",
        "neither = 0\n",
        "\n",
        "#Create confusion matrix\n",
        "confusion_data = np.array([\n",
        "    [len(both), len(only_expert)],\n",
        "    [len(only_learned), 0]\n",
        "])\n",
        "\n",
        "#confusion matrix visualisation\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(confusion_data, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['In Learned DAG', 'Not in Learned DAG'],\n",
        "            yticklabels=['In Expert DAG', 'Not in Expert DAG'],\n",
        "            cbar_kws={'label': 'Number of Edges'})\n",
        "plt.title('Edge Comparison: Expert DAG vs Learned DAG', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Learned DAG (2.2)', fontsize=12)\n",
        "plt.ylabel('Expert DAG (2.1)', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\" summary of Edge comparison \")\n",
        "\n",
        "print(f\"Edges in both models: {len(both)}\")\n",
        "print(f\"Edges only in Expert DAG: {len(only_expert)}\")\n",
        "print(f\"Edges only in Learned DAG: {len(only_learned)}\")\n",
        "print(f\"Total Expert edges: {len(expert_set)}\")\n",
        "print(f\"Total Learned edges: {len(learned_set)}\")\n",
        "print(f\"Agreement rate: {len(both)/len(expert_set|learned_set)*100:.1f}%\")\n",
        "\n",
        "\n",
        "print(\"Edges in both models\")\n",
        "\n",
        "for edge in sorted(both):\n",
        "    print(f\"{edge[0]} → {edge[1]}\")\n",
        "\n",
        "print(\"Edges only in expert DAG (Missed by algorithm)\")\n",
        "\n",
        "for edge in sorted(only_expert):\n",
        "    print(f\"- {edge[0]} → {edge[1]}\")\n",
        "\n",
        "\n",
        "print(\"Edges only in leanerd DAG (Found by algorithm)\")\n",
        "\n",
        "for edge in sorted(only_learned):\n",
        "    print(f\"+ {edge[0]} → {edge[1]}\")"
      ],
      "metadata": {
        "id": "eMEo4OoeR7YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Extract edges\n",
        "edges_bn = set(bn_model['model'].edges())\n",
        "edges_expert = set(G_expert.edges())\n",
        "\n",
        "# Identify common and unique edges\n",
        "common_edges = edges_bn & edges_expert\n",
        "unique_bn = edges_bn - edges_expert\n",
        "unique_expert = edges_expert - edges_bn\n",
        "\n",
        "# Create graphs\n",
        "G_bn = nx.DiGraph()\n",
        "G_bn.add_nodes_from(bn_model['model'].nodes())\n",
        "G_bn.add_edges_from(edges_bn)\n",
        "\n",
        "G_expert = nx.DiGraph()\n",
        "G_expert.add_nodes_from(G_expert.nodes())\n",
        "G_expert.add_edges_from(edges_expert)\n",
        "\n",
        "# Define layouts\n",
        "# You can experiment here\n",
        "pos_bn = nx.spring_layout(G_bn, seed=42, k=2)\n",
        "pos_expert = nx.spring_layout(G_bn, seed=7, k=2)\n",
        "\n",
        "#Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "#BN Model\n",
        "nx.draw_networkx_nodes(G_bn, pos_bn, node_color='#B3A7FF', node_size=900, ax=axes[0])\n",
        "nx.draw_networkx_labels(G_bn, pos_bn, font_color='black', font_size=8, ax=axes[0])\n",
        "nx.draw_networkx_edges(G_bn, pos_bn, edgelist=common_edges, edge_color='#FF9800', arrows=True, arrowstyle='-|>', arrowsize=23, width=2, ax=axes[0])\n",
        "nx.draw_networkx_edges(G_bn, pos_bn, edgelist=unique_bn, edge_color='#2196F3', arrows=True, arrowstyle='-|>', arrowsize=23, width=2, style='dashed', ax=axes[0])\n",
        "axes[0].set_title(\"Learned Model\", fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "#Expert Model\n",
        "nx.draw_networkx_nodes(G_expert, pos_expert, node_color='#B3A7FF', node_size=900, ax=axes[1])\n",
        "nx.draw_networkx_labels(G_expert, pos_expert, font_color='black', font_size=8, ax=axes[1])\n",
        "nx.draw_networkx_edges(G_expert, pos_expert, edgelist=common_edges, edge_color='#FF9800', arrows=True, arrowstyle='-|>', arrowsize=23, width=2, ax=axes[1])\n",
        "nx.draw_networkx_edges(G_expert, pos_expert, edgelist=unique_expert, edge_color='#2196F3', arrows=True, arrowstyle='-|>', arrowsize=23, width=2, style='dashed', ax=axes[1])\n",
        "axes[1].set_title(\"Expert Model\", fontsize=12)\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Legend\n",
        "common_patch = mpatches.Patch(color='#FF9800', label='Common Edge (in both DAGs)')\n",
        "unique_patch = mpatches.Patch(color='#2196F3', label='Unique Edge (only in this DAG)')\n",
        "node_patch = mpatches.Patch(color='#B3A7FF', label='Node')\n",
        "\n",
        "fig.legend(handles=[node_patch, common_patch, unique_patch],\n",
        "           loc='lower center', ncol=3, fontsize=10, frameon=False)\n",
        "\n",
        "plt.suptitle(\"Comparison of Learned vs Expert DAGs\", fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gmXbPdT7iR2O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build your Bayesian Networks (Parameter Learning)"
      ],
      "metadata": {
        "id": "rQdHLLR9Qh0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPDs (parameters) using Maximum Likelihood Estimation (MLE)\n",
        "bn_model_learned = bn.parameter_learning.fit(bn_model, retail, methodtype='maximumlikelihood')\n",
        "bn.print_CPD(bn_model_learned)"
      ],
      "metadata": {
        "id": "IdJEPX_j2c01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Display the full CPD table as a DataFrame\n",
        "cpd_df = bn_model_learned['Purchase_Intention']\n",
        "print(cpd_df)\n",
        "\n",
        "# Option 2: Remove display limits to see all rows\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "print(bn_model_learned['Purchase_Intention'])\n",
        "\n",
        "# Option 3: Verify probabilities sum to 1 for each Perceived_Value\n",
        "verification = cpd_df.groupby('Perceived_Value')['p'].sum()\n",
        "print(\"\\nProbability sums for each Perceived_Value:\")\n",
        "print(verification)\n",
        "\n",
        "# Option 4: See a specific Perceived_Value in detail\n",
        "pv_example = cpd_df[cpd_df['Perceived_Value'] == 0.8325]\n",
        "print(f\"\\nAll Purchase_Intention values when Perceived_Value = 0.8325:\")\n",
        "print(pv_example)\n",
        "print(f\"Sum of probabilities: {pv_example['p'].sum()}\")"
      ],
      "metadata": {
        "id": "P6lnPxfi6Oc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference\n",
        "q1 = bn.inference.fit(bn_model, variables=['Purchase_Intention'], evidence=None)\n",
        "print(q1)"
      ],
      "metadata": {
        "id": "fUmEv3vr8uxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f8c2066f-952c-4c68-a4fa-5f5244847af9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'keys'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3558082423.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Purchase_Intention'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bnlearn/inference.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, variables, evidence, to_df, elimination_order, joint, groupby, plot, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[bnlearn] >Error: [variables] should match names in the model (Case sensitive!)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[bnlearn] >Error: [evidence] should match names in the model (Case sensitive!)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[bnlearn] >Variable Elimination.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get the inferences for the 4 Combinations mentioned in the Assignment"
      ],
      "metadata": {
        "id": "f_fVnwfUQnFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combination 1"
      ],
      "metadata": {
        "id": "gbgsPiazRP-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide multiple pieces of evidence\n",
        "q_multiple_evidence = bn.inference.fit(bn_model_learned,\n",
        "                                       variables=['Perceived_Value'],\n",
        "                                       evidence={'Purchase_Intention': 1.0})\n",
        "print(q_multiple_evidence)\n",
        "# The result is conditioned on both observations simultaneously.\n"
      ],
      "metadata": {
        "id": "p_LkHEtbRO9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155442cf-8540-4ef7-8211-1f7db1231bdd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bnlearn] >Variable Elimination.\n",
            "+----+-------------------+-----------+\n",
            "|    |   Perceived_Value |         p |\n",
            "+====+===================+===========+\n",
            "|  0 |            0      | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  1 |            0.1675 | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  2 |            0.25   | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  3 |            0.3325 | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  4 |            0.4175 | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  5 |            0.5    | 0         |\n",
            "+----+-------------------+-----------+\n",
            "|  6 |            0.5825 | 0.0357143 |\n",
            "+----+-------------------+-----------+\n",
            "|  7 |            0.6675 | 0.0357143 |\n",
            "+----+-------------------+-----------+\n",
            "|  8 |            0.75   | 0.125     |\n",
            "+----+-------------------+-----------+\n",
            "|  9 |            0.8325 | 0.321429  |\n",
            "+----+-------------------+-----------+\n",
            "| 10 |            0.9175 | 0.25      |\n",
            "+----+-------------------+-----------+\n",
            "| 11 |            1      | 0.232143  |\n",
            "+----+-------------------+-----------+\n",
            "\n",
            "Summary for variables: ['Perceived_Value']\n",
            "Given evidence: Purchase_Intention=1.0\n",
            "\n",
            "Perceived_Value outcomes:\n",
            "- Perceived_Value: 0.0 (0.0%)\n",
            "- Perceived_Value: 0.16749999999999998 (0.0%)\n",
            "- Perceived_Value: 0.25 (0.0%)\n",
            "- Perceived_Value: 0.3325 (0.0%)\n",
            "- Perceived_Value: 0.4175 (0.0%)\n",
            "- Perceived_Value: 0.5 (0.0%)\n",
            "- Perceived_Value: 0.5825 (3.6%)\n",
            "- Perceived_Value: 0.6675 (3.6%)\n",
            "- Perceived_Value: 0.75 (12.5%)\n",
            "- Perceived_Value: 0.8325 (32.1%)\n",
            "- Perceived_Value: 0.9175 (25.0%)\n",
            "- Perceived_Value: 1.0 (23.2%)\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value                      |   phi(Perceived_Value) |\n",
            "+======================================+========================+\n",
            "| Perceived_Value(0.0)                 |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.16749999999999998) |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.25)                |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.3325)              |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.4175)              |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.5)                 |                 0.0000 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.5825)              |                 0.0357 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.6675)              |                 0.0357 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.75)                |                 0.1250 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.8325)              |                 0.3214 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(0.9175)              |                 0.2500 |\n",
            "+--------------------------------------+------------------------+\n",
            "| Perceived_Value(1.0)                 |                 0.2321 |\n",
            "+--------------------------------------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text explaining the inference of the combination"
      ],
      "metadata": {
        "id": "Wqn18DY6RUwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combination 2"
      ],
      "metadata": {
        "id": "r8L1gN9VRa6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide multiple pieces of evidence\n",
        "q_multiple_evidence = bn.inference.fit(bn_model_learned,\n",
        "                                       variables=['Price_Sensitivity'],\n",
        "                                       evidence={'Purchase_Intention': 0.75 })\n",
        "print(q_multiple_evidence)\n",
        "# The result is conditioned on both observations simultaneously.\n"
      ],
      "metadata": {
        "id": "LdcDV1BJRa6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047305f3-4de5-414a-d072-1a0eff49ee7e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bnlearn] >Variable Elimination.\n",
            "+----+---------------------+------------+\n",
            "|    |   Price_Sensitivity |          p |\n",
            "+====+=====================+============+\n",
            "|  0 |              0      | 0.0926666  |\n",
            "+----+---------------------+------------+\n",
            "|  1 |              0.0825 | 0.0126797  |\n",
            "+----+---------------------+------------+\n",
            "|  2 |              0.1675 | 0.0131125  |\n",
            "+----+---------------------+------------+\n",
            "|  3 |              0.25   | 0.0647201  |\n",
            "+----+---------------------+------------+\n",
            "|  4 |              0.3325 | 0.129914   |\n",
            "+----+---------------------+------------+\n",
            "|  5 |              0.4175 | 0.0582173  |\n",
            "+----+---------------------+------------+\n",
            "|  6 |              0.5    | 0.151191   |\n",
            "+----+---------------------+------------+\n",
            "|  7 |              0.5825 | 0.161635   |\n",
            "+----+---------------------+------------+\n",
            "|  8 |              0.6675 | 0.0829417  |\n",
            "+----+---------------------+------------+\n",
            "|  9 |              0.75   | 0.160225   |\n",
            "+----+---------------------+------------+\n",
            "| 10 |              0.8325 | 0.00891304 |\n",
            "+----+---------------------+------------+\n",
            "| 11 |              0.9175 | 0.0252478  |\n",
            "+----+---------------------+------------+\n",
            "| 12 |              1      | 0.0385375  |\n",
            "+----+---------------------+------------+\n",
            "\n",
            "Summary for variables: ['Price_Sensitivity']\n",
            "Given evidence: Purchase_Intention=0.75\n",
            "\n",
            "Price_Sensitivity outcomes:\n",
            "- Price_Sensitivity: 0.0 (9.3%)\n",
            "- Price_Sensitivity: 0.08250000000000002 (1.3%)\n",
            "- Price_Sensitivity: 0.16749999999999998 (1.3%)\n",
            "- Price_Sensitivity: 0.25 (6.5%)\n",
            "- Price_Sensitivity: 0.3325 (13.0%)\n",
            "- Price_Sensitivity: 0.4175 (5.8%)\n",
            "- Price_Sensitivity: 0.5 (15.1%)\n",
            "- Price_Sensitivity: 0.5825 (16.2%)\n",
            "- Price_Sensitivity: 0.6675 (8.3%)\n",
            "- Price_Sensitivity: 0.75 (16.0%)\n",
            "- Price_Sensitivity: 0.8325 (0.9%)\n",
            "- Price_Sensitivity: 0.9175 (2.5%)\n",
            "- Price_Sensitivity: 1.0 (3.9%)\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity                      |   phi(Price_Sensitivity) |\n",
            "+========================================+==========================+\n",
            "| Price_Sensitivity(0.0)                 |                   0.0927 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.08250000000000002) |                   0.0127 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.16749999999999998) |                   0.0131 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.25)                |                   0.0647 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.3325)              |                   0.1299 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.4175)              |                   0.0582 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.5)                 |                   0.1512 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.5825)              |                   0.1616 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.6675)              |                   0.0829 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.75)                |                   0.1602 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.8325)              |                   0.0089 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(0.9175)              |                   0.0252 |\n",
            "+----------------------------------------+--------------------------+\n",
            "| Price_Sensitivity(1.0)                 |                   0.0385 |\n",
            "+----------------------------------------+--------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text explaining the inference of the combination"
      ],
      "metadata": {
        "id": "63nC-fKLRa6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combination 3"
      ],
      "metadata": {
        "id": "dtZp8rA7Rcem"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZM7tk7JRcem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text explaining the inference of the combination"
      ],
      "metadata": {
        "id": "hcC0QYtNRcem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combination 4"
      ],
      "metadata": {
        "id": "NP5wX5pLReKb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZQNk87YReKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text explaining the inference of the combination"
      ],
      "metadata": {
        "id": "_oPGbVrEReKc"
      }
    }
  ]
}